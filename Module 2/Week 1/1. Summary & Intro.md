# Summary of Introduction to BIG DATA:
## 1.  Launch of big data era:
  Big torrent of data + cloud computing = Big DATA
## 2. Three major sources of big DATA
  - Machine Generated Data:
    - Web logs, environmental sensors, personal health trackers
  - Human Generated Data:
    - Vast amount of social media data - status, tweets, photos, and videos
  - Organization generated DATA
    - traditional types of data:
      - transaction information databases
      - structred data open source and databases

Data can be structured semi structured and unstructured

## 3. Characteristics of big data: | Challenges
  - **Main dimensions**:
    - Volume - Size
    - Variety - Complexity
      - Velocity- Speed
  - Others:
    - Veracity - Quality (unmeasurable certainties and truthfulness of data)
    - Valence - Connectedness of data
Big data engineering + scalable data science = generates value of big data.

## 4. Data Science
  - turns big data into insights and even actions.
  - basis for emperical research - data is used to induce information on observation(data - big data)
  - Insight - term to refer data products of data Science
    - extracted from comb of exploratory analysis and modelling
  - Not static one time analysis.
  - involves a process where models generated giving insights are constantly improved through further emperical evidence and iterations.
  - Process of improvement:
    - 2 distinct activities:
      - Big data engineering
      - Computational Big data Science
    - Detailed activites of data science Process
      - Acquire - retieval of data
        - Finding, accessing, acquiring and moving databases
        - Includes identification of and authenticated access to all related data and transportation of data from sources to dest.
        - Includes ways to subset and match the data to region or times of interest: Geospatial querying.
      - Prepare
        - Divided into:
          - 1. Explore (Exploratory analysis)
            - Data to understand nature, quality and format
            - Called prepare because it requires samples of data to get these understandings.
          - 2. Pre-Process (Pre-processing data for analysis)
            - Clean / subset / filter data
              - Creating data that is understandable by programs by modelling raw data into defined data model.
            - Integrate - if multiple data sets involved.
            - Package - by using specific data format.
      - Analyze
        - Selection of analytical techniques
        - Building model of data
        - Analyzing results.
        > Can require couple of steps on its own / go back to step 1.

      - Report
        - Evaluation of analytical results
        - Presenting in visual way
        - creating reports wrt success criteria
        - Activities:
          - Interpret
          - Summarize
          - Visualize
          - Post-process
      - Act
        - First reason to do data scince:
          - Purpose
        - Reporting insights from analysis and determining actions from insights based on the purpose you initially defined is what we refer to as the act step.

    > Data science happens at the boundary of all these steps<br><br>
    > Please note that this is an iterative process and findings from one step may require previous steps to be repeated, but need information, leading for further exploration and application of these steps.

## 5. Hadoop
  - Functionalities
    - 1. Provide scalability to store large volumes of data in commoditiy hardware
    - 2. Handle fault tolerance
    - 3. Optimized for a variety data types - handles variety, compressing files, graphs of social networks, streaming sensor data and raster images.
    - 4. Facilitate a shared environment - For multiple jobs to occur simultaneously.
    - 5. Provide Value / Support - hadoop is backed by large and active community
  - Hadoop distributed file system(HDFS)
    - Foundation for many big data frameword since it provides scalabile and reliable storage.
    -  As size of data increases, h/w can be added to hdfs to increase storage capacity - **Scaling out of your resouces**
  - Hadoop YARN
    - Provides flexible scheduling and resource management over the hdfs storage
    - MapReduce - Programming model that simplifies parallel programming.
      - Only assumes a limited model to express data:
      **Following are hadoop stack components**
        - Hive - FB (SQL like queries) and Pig - Yahoo (Dataflow scripting) 2 additional programming model to augment map reduce to relational algebra and data flow modelling respectively.
        - Giraph - processes large scale graphs efficiently (FB uses giraph to analyze social graph of users.)
        - Storm spark and flink - real time and in memory processing of big data on top of YARN resource scheduler and HDFS.
          - In memory processing - runs big data application faster (100x better performance for some task)
        - For collection of key values and large sparse tables, use NOSQL projects -
          - Casandra - FB
            - Used along with Hbase for messaging platform.
          - MongoDB
          - Hbase
      - zOOKEEPER FOR MANAGEMENT:
        - Synchronization, configuration and high availability of all these tools of centralized management systems
        - Created by yahoo to wrangle services named after animals.
