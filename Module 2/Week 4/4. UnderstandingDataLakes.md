# Understanding Data Lakes
**Refer slides simultaneously**

  - We need to ingest this data (of various formats and models) into a fast and scalable storage system that is flexible enough to serve many current and future analytical processes.
  - Traditional data warehouse with strict data models and formats don't fit into these challenges (of streaming and batch applications) and hence we need the concept of data Lakes.

## What is a data lake?
- Simply speaking, a data lake is a part of a big data infrastructure that many streams can flow into and get stored for processing in their original form
-  We can think of it as a massive storage depository with huge processing power and ability to handle a very large number of concurrence, data management and analytical tasks.
- Working:
  - The data gets loaded from its source ->
  - Stored in its native format until it is needed ->
  - at which time the applications can freely read the data and add structure to it.
- **This is what we call schema-on-read**
- Traditional Data warehouse approach - **Schema-on-write**:
  - In a traditional data warehouse, the data is loaded into the warehouse after transforming it into a well defined and structured format.
  - This is what we call schema on write. Any application using the data needs to know this format in order to retrieve and use the data.
  - In this approach, data is not loaded into the warehouse unless there is a use for it.
  - However, **schema on read approach of data lakes ensures all data is stored for a potentially unknown use at a later time.**

- So how is a data lake different from a data warehouse?
- |Data Lake|Data Warehouse|
|:--|:---|
  Stores data as flat files with unique identifiers <br>( Referred to as **Object storage in big data system** )| Stores data in a heirarchical file system with well defined structure|  
  Each data stored as BLOB (Binary large object) and assigned a UID|   |  
  Each data object is tagged with a number of metadata tags|   |  
Data searched using these metadata tags  |   |  
  - From a users perspective, metadata is stored is not a problem as long as it is accessible when needed.
  - In Hadoop data architectures, data is loaded into HDFS and processed using the appropriate data management and analytical systems on commodity clusters.
  - Selection of tools based on problem being solved and data format being accessed

Summary:
To summarize
- a data lake is a storage architecture for big data collection and processing.
- It enables collection of all data suitable for analysis today and potentially in the future.
- Regardless of the data source, structure, and format it supports storage of data and transforms it only when it is needed.
- A data lake ideally supports all parts of the user base to benefit from this architecture, including business, storage, analytics and computing experts.
- Finally, And perhaps most importantly, data lakes are infrastructure components within a big data architecture that can evolve over time based on application-specific needs.
**Hands on of week 4 after this**
